{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways of avoiding overfitting is using cross validation, that helps in estimating the error over test set, and in deciding what parameters work best for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "\n",
    "2. Split the dataset into k groups\n",
    "\n",
    "3. For each unique group:\n",
    "\n",
    "    1.Take the group as a hold out or test data set\n",
    "    \n",
    "    2.Take the remaining groups as a training data set\n",
    "    \n",
    "    3.Fit a model on the training set and evaluate it on the test set\n",
    "    \n",
    "    4. Retain the evaluation score and discard the model\n",
    "    \n",
    "4.Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "- Representative: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "\n",
    "- k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "\n",
    "- k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0 1 3 4 5 7], test: [2 6 8]\n",
      "train: [2 3 4 5 6 8], test: [0 1 7]\n",
      "train: [0 1 2 6 7 8], test: [3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn k-fold cross-validation\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.model_selection import KFold\n",
    "# data sample\n",
    "data = np.arange(9)\n",
    "# prepare cross validation\n",
    "kfold = KFold(3, True, 1)\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations on Cross-Validation\n",
    "There are a number of variations on the k-fold cross validation procedure.\n",
    "\n",
    "Three commonly used variations are as follows:\n",
    "\n",
    "- Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "- LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n",
    "- Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
    "- Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of train/test split:\n",
    "\n",
    "1. This runs K times faster than Leave One Out cross-validation because K-fold cross-validation repeats the train/test split K-times.\n",
    "\n",
    "2. Simpler to examine the detailed results of the testing process.\n",
    "\n",
    "#### Advantages of cross-validation:\n",
    "\n",
    "1. More accurate estimate of out-of-sample accuracy.\n",
    "\n",
    "2. More “efficient” use of data as every observation is used for both training and testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
