{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an algorithm. That has recently been dominating applied machine learning.\n",
    "XGBoost Algorithm is an implementation ofgradient boosted decision trees. That was designed for speed and performance.\n",
    "Basically, XGBoosting is a type of software library. That you can download and install on your machine. Then have to access it from a variety of interfaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. XGBoost Features ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Model Features :\n",
    "XGBoost model implementation supports the features of the scikit-learn and R implementations. Three main forms of gradient boosting are supported:\n",
    "\n",
    "Gradient Boosting :\n",
    "This is also called as gradient boosting machine including the learning rate.\n",
    "\n",
    "Stochastic Gradient Boosting :\n",
    "This is the boosting with sub-sampling at the row, column, and column per split levels.\n",
    "\n",
    "Regularized Gradient Boosting :\n",
    "It includes boosting with both L1 and L2 regularization.\n",
    "Read more about Machine Learning Algorithms\n",
    "\n",
    "b. System Features\n",
    "For use of a range of computing environments this library provides:\n",
    "Parallelization of tree construction using all of your CPU cores during training.\n",
    "Distributed Computing for training very large models using a cluster of machines.\n",
    "Out-of-Core Computing for very large datasets that don’t fit into memory.\n",
    "Cache Optimization of data structures and algorithm to make the best use of hardware.\n",
    "\n",
    "c. Algorithm Features\n",
    "For efficiency of computing time and memory resources, we use XGBoost algorithm. Also, this was designed to make use of available resources to train the model.\n",
    "Some key algorithm implementation features include:\n",
    "Sparse aware implementation with automatic handling of missing data values.\n",
    "Block structure to support the parallelization of tree construction.\n",
    "Continued training so that you can further boost an already fitted model on new data.\n",
    "XGBoost is free open source software. That is available for use under the permissive Apache-2 license.\n",
    "Read about Applications of Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Why XGBoosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two reasons to use XGBoosting Algorithms are also the two goals of the project:\n",
    "\n",
    "a. XGBoost Execution Speed:\n",
    "\n",
    "When we compare XGBoosting to implementations of gradient boosting, it’s so fast.\n",
    "It compares XGBoost to other implementations of gradient boosting and bagged decision trees. Also, he wrote up his results in May 2015 in the blog post titled. That is “Benchmarking Random Forest Implementations“.\n",
    "Moreover, it provides all the code on GitHub and a more extensive report of results with hard numbers.\n",
    "\n",
    "b. XGBoost Model Performance:\n",
    "\n",
    "It dominates structured datasets on classification and regression predictive modeling problems.\n",
    "The evidence is that it is a go-to algorithm for competition winners. That is based on the Kaggle competitive data science platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Why XGBoosting is good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Flexibility\n",
    "\n",
    "XGBoosting supports user-defined objective functions with classification, regression and ranking problems. We use an objective function to measure the performance of the model. That is given a certain set of parameters. Furthermore, it supports user-defined evaluation metrics as well.\n",
    "\n",
    "b. Availability\n",
    "\n",
    "As it is available for programming languages such as R, Python, Java, Julia, and Scala.\n",
    "\n",
    "c. Save and Reload\n",
    "\n",
    "We can easily save our data matrix and model and reload it later. Let suppose, if we have a large dataset, we can simply save the model. Further, we use it in future instead of wasting time redoing the computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. XGBoost Algorithm working With Main Interfaces..?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C++, Java and JVM languages.\n",
    "\n",
    "Julia.\n",
    "\n",
    "Command Line Interface.\n",
    "\n",
    "R interface as well as a model in the caret package.\n",
    "\n",
    "Python interface along with integrated model in scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Xgboost Algorithm – Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. General Parameters\n",
    "\n",
    "b. Booster Parameters\n",
    "\n",
    "c. Linear Booster Specific Parameters\n",
    "\n",
    "d. Learning Task Parameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Is XGBoost better than random forest?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folks know that gradient-boosted trees generally perform better than a random forest, although there is a price for that: GBT have a few hyperparams to tune, while random forest is practically tuning-free. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Why is XGBoost better than GBM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quote from the author of xgboost : Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.How does XG boost work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. ... It's called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.Why does XGBoost work so well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a scalable and accurate implementation of gradient boosting machines and it has proven to push the limits of computing power for boosted trees algorithms as it was built and developed for the sole purpose of model performance and computational speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.How does XGBoost handle missing values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost decides at training time whether missing values go into the right or left node. It chooses which to minimise loss. If there are no missing values at training time, it defaults to sending any new missings to the right node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.Is XGBoost a classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) ... A wide range of applications: Can be used to solve regression, classification, ranking, and user-defined prediction problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
