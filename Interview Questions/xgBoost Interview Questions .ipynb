{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q1) Which of the following tree based algorithm uses some parallel (full or partial) implementation?\n",
    "A) Random Forest\n",
    "B) Gradient Boosted Trees\n",
    "C) XGBOOST\n",
    "D) Both A and C\n",
    "E) A, B and C\n",
    "\n",
    "Solution: D\n",
    "\n",
    "\n",
    "Only Random Forest and XGBoost have parallel implementations.\n",
    "Random Forest is very easy to parallelize, where as XGBoost can have partially parallel implementation. In Random Forest, all trees grows parallel and finally ensemble the output of each tree .\n",
    "Xgboost doesn’t run multiple trees in parallel like Random Forest, you need predictions after each tree to update gradients. Rather it does the parallelization WITHIN a single tree to create branches independently.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q2) Which of the following is a mandatory data pre-processing step(s) for XGBOOST?\n",
    "1. Impute Missing Values\n",
    "2. Remove Outliers\n",
    "3. Convert data to numeric array / sparse matrix\n",
    "4. Input variable must have normal distribution\n",
    "5. Select the sample of records for each tree/ estimators\n",
    "\n",
    "A) 1 and 2\n",
    "B) 1, 2 and 3\n",
    "C) 3, 4 and 5\n",
    "D) 3\n",
    "E) 5\n",
    "F) All\n",
    "\n",
    "Solution: D\n",
    "\n",
    "\n",
    "XGBoost is doesn’t require most of the pre-processing steps, so only converting data to numeric is required among of the above listed steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q3) Let’s say we have m number of estimators (trees) in a XGBOOST model. Now, how many trees will work on bootstrapped data set?\n",
    "A) 1\n",
    "B) m-1\n",
    "C) m\n",
    "D) Can’t say\n",
    "E) None of the above\n",
    "\n",
    "Solution: C\n",
    "\n",
    "All the trees in XGBoost will work on bootstrapped data. Therefore, option C is true"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q4) Which of the following statement is correct about XGBOOST parameters:\n",
    "1. Learning rate can go upto 10\n",
    "2. Sub Sampling / Row Sampling percentage should lie between 0 to 1\n",
    "3. Number of trees / estimators can be 1\n",
    "4. Max depth can not be greater than 10\n",
    "\n",
    "A) 1\n",
    "B) 1 and 3\n",
    "C) 1, 3 and 4\n",
    "D) 2 and 3\n",
    "E) 2\n",
    "F) 4\n",
    "\n",
    "Solution: D\n",
    "\n",
    "    1 and 4 are wrong statements, whereas 2 and 3 are correct. Therefore D is true. Refer this article.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q5) Can you name at least two boosting algorithms in machine learning?\n",
    "\n",
    "Solution : \n",
    "4 Boosting Algorithms in Machine Learning\n",
    "    Gradient Boosting Machine (GBM)\n",
    "    Extreme Gradient Boosting Machine (XGBM)\n",
    "    LightGBM\n",
    "    CatBoost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q6) LightGBM vs XGBoost ?\n",
    "\n",
    "Solution : \n",
    "    So now let’s compare LightGBM with XGBoost by applying both the algorithms to a dataset and then comparing the performance.\n",
    "   \n",
    "There has been only a slight increase in accuracy and auc score by applying Light GBM over XGBOOST but there is a significant difference in the execution time for the training procedure. Light GBM is almost 7 times faster than XGBOOST and is a much better approach when dealing with large datasets.\n",
    "This turns out to be a huge advantage when you are working on large datasets in limited time competitions.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q7) XGBoost in Python \n",
    "\n",
    "Solution :\n",
    "    That XGBoost is a library for developing fast and high performance gradient boosting tree models.\n",
    "    That XGBoost is achieving the best performance on a range of difficult machine learning tasks.\n",
    "    That you can use this library from the command line, Python and R and how to get started."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q8) What is XGBoost?\n",
    "\n",
    "Answer : XGBoost stands for eXtreme Gradient Boosting.\n",
    "The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q9) Which main interfaces oes Xgboost supports ?\n",
    "\n",
    "Answer : \n",
    "    \n",
    "    Command Line Interface (CLI).\n",
    "    C++ (the language in which the library is written).\n",
    "    Python interface as well as a model in scikit-learn.\n",
    "    R interface as well as a model in the caret package.\n",
    "    Julia.\n",
    "    Java and JVM languages like Scala and platforms like Hadoop."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q10) XGBoost Features ?\n",
    "\n",
    "\n",
    "Answer : \n",
    "    \n",
    "    Model Features  \n",
    "        Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n",
    "        Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\n",
    "        Regularized Gradient Boosting with both L1 and L2 regularization.\n",
    "    System Features:\n",
    "        Parallelization of tree construction using all of your CPU cores during training.\n",
    "        Distributed Computing for training very large models using a cluster of machines.\n",
    "        Out-of-Core Computing for very large datasets that don’t fit into memory.\n",
    "        Cache Optimization of data structures and algorithm to make best use of hardware.\n",
    "    Algorithm Features:\n",
    "        Sparse Aware implementation with automatic handling of missing data values.\n",
    "        Block Structure to support the parallelization of tree construction.\n",
    "        Continued Training so that you can further boost an already fitted model on new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
